\documentclass[a4paper,11pt]{report}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
% \usepackage{ltablex}
\usepackage{setspace}
\usepackage{datetime}
\usepackage{url}
\usepackage{cite}
\usepackage{tikz}
\usepackage{pgf-umlsd}
\usepackage{geometry}
\geometry{
 a4paper,
 left=1in,
 top=1in,
}
\newcommand{\titles}{\\\vspace{1cm}}
\newcommand{\cn}{\textbf{[Citation Needed]}}
\newcommand{\objv}[3]{\item \textbf{OBJ-#1}: \textit{#2}\\#3}
\newcommand{\objitem}[4]{\begin{tabularx}{\textwidth}{|l|cr|} \hline \textbf{OBJ-#1} & #2 & #3 \\ \hline \multicolumn{3}{|X|}{#4} \\ \hline \end{tabularx}}
\newcommand{\riskitem}[4]{\begin{tabularx}{\textwidth}{|l|cr|} \hline \textbf{RSK-#1} & #2 & #3 \\ \hline \multicolumn{3}{|X|}{#4} \\ \hline \end{tabularx}}
\newcommand{\objts}{\multicolumn{3}{X}{}\\}

% \keepXColumns
\pdfinfo{%
  /Title    (Deliverable 1: Final Year Dissertation)
  /Author   (Leon McGregor)
}

\begin{document}

{\centering\Large
\includegraphics[width=0.4\textwidth]{../hwu.png}\titles
Deliverable 1: Final Year Dissertation\titles
{\huge\bfseries Web platform for code peer-testing\titles}
L\'eon \textsc{McGregor} - H00152968\titles
{\large\textit{Supervisor}\\}
Manuel \textsc{Maarek}\titles
{\large\textit{Second Reader}\\}
Ron \textsc{Petrick}\\
\vfill
}

\pagebreak

\tableofcontents


\pagebreak
\doublespacing


\section*{Abstract}
This project aims to build a protoype for a peer assessment website. The website would be used in an educational environment, for example in a Computer Science course, and be used to let students easily perform peer assessment. This document will detail some of the existing relevant research, and detail a plan for development of the new website.

\vfill

\section*{Declaration}
I, L\'eon McGregor confirm that this work submitted for assessment is my own and is expressed in my own words. Any uses made within it of the works of other authors in any form (e.g., ideas, equations, figures, text, tables, programs) are properly acknowledged at any point of their use. A list of the references employed is included.\par
Signed: L\'eon McGregor\par
Date: \today

\vfill

\section*{Acknowledgements}

\pagebreak

\pagestyle{headings}
\markright{L\'eon McGregor - Deliverable 1: Final Year Dissertation}




\section{Introduction}
The aim of this project is to investigate and develop a website which will allow for peer assessment within a computer science course. This website will aim to provide an easy way for lecturers to request programming assignments, and for students to respond by submitting completed assignments, and then testing each others code. The peer assessment will allow the students to get fast and easy to understand feedback for assignments, and could be used as a starting point for grading of assignments.\par
The website will need to handle this code in a secure manner, to prevent either intentional or accidental malicious code from running.\par
Much research has been already completed in the study of Peer Assessment, from the front of being used as an educational tool, to being used as a basis for competition between programmers. Some relevant research into this will be analysed here.

\subsection{Primary Aim}
The primary aim of this project is to design, build and test a prototype website for peer assessment. I will then evaluate the website, to determine if it can successfully automate and enhance the peer assessment experience. This task can be broken down to the following main objectives:
\begin{enumerate}
\item Build a website that automates peer assessment - That is to say, it combines file upload, viewing, testing and providing feedback in one place
\item Evaluate whether or not this website enhances peer assessment - By comparing a peer assessment process with and without this site, determine if it can offer any advantages to the users
\end{enumerate}
For the initial prototype of this website and evaluation, I would focus on using a simple python programming exercise.
\subsection{Overview of background research}
%summarize the research here

\chapter{Background}
There has been lots of work done in looking into the use of peer testing. Introduce the various things I looked at here, as an overview.
\subsection{sadler-impact-2006}
Peer assessment is a process in which students work to assess each others work. Sadler \& Good \cite{sadler_impact_2006} have suggested the following concepts that peer assessment can help with
\begin{itemize}
 \item Peer assessment is more immediate, so students can get more feedback, and sooner
 \item Students performing marking can reduce the workload for teachers
 \item The process of checking and thinking about another students answer can improve a students own understanding
 \item Peer assessment can help students better understand testing and can become aware of their own strengths and weaknesses
 \item Following peer assessment students can gain an improved attitude towards the process of learning as a whole
\end{itemize}
From this, we can see that using peer assessment is definitely a desirable choice. However, given that it requires the participation of students, and sharing of work, this might be tricky to achieve.\cn?

\subsection{li-role-2016}
When developing a peer assessment system, or an assessment system of any kind, it is important to know whether or not to reveal the identities of those being assessed. Very often, assessment marking is done anonymously\cn.A study was conducted\cite{li_role_2016} that aimed to investigate just how effective anonymity is when it comes to making peer assessment more effective, and whether any negative impact from a lack of anonymity can be mitigated.\par
This quasi-experimental study was conducted with some in-training teachers, and aimed to see which is the most effective method of / using / while conducting a peer assessment exercise: Having assessors and assessees know each others identities, remain anonymous, or know identities while having received training.\par
One concern I have with this study is that it didn't cover the case of being anonymous and getting training. This was because the training was intended as a fallback for when anonymity is not possible. But this does invite the question of just how effective would anonymity be if training were offered as well.\par

\subsection{davies-computerized-2000}
A study by Phil Davies\cite{davies_computerized_2000} was conducted using a peer testing system in a university communications and networking module. This peer testing system aimed to help students easily provide feedback on reports, in addition to attempting to locate plagiarism.\par
The CAP (Computerized Assessment with Plagiarism) provided an interface to let students read the report of another student. They could then provide feedback on the report and follow references on the report to identify any \textit{copy \& paste} plagiarism. This CAP system is of particular interest as the interface described in Davies' paper is quite similar to what I would aim to test in this project.\par
This experiment followed the results students achieved over a sequence of 4 tasks:
\begin{enumerate}
\item A Report
\item A multiple choice test
\item A period for peer marking the reports between students
\item A final multiple choice test
\end{enumerate}
The experiment looked to see if the peer marking was at all helpful in improving the marks of students between the two tests. The results found that the peer marking was very useful for students who had initially performed poorly. Student comments on the experience would suggest that they found the peer assessment process both enjoyable and informative\cn. There is also evidence from this feedback indicating the importance of maintaining anonymity, as some students felt that the feedback given to them was unfair \cn.

\subsection{smith-using-2012}
A study performed in a classroom environment by Smith et al.\cite{smith_using_2012} focused more on the use of peer assessment as a tool for teaching testing of code. Over the course of this study, which took place using coursework from a 12-week university course, the following was completed for each coursework: Submitting solutions, then submitting peer reviews (which includes a description of the testing that they performed on another solution, and the results of this), and then a review of the peer assessment (including what was learned, an evaluation of feedback on their own solution, and optionally a corrected solution).\par
One particularly noteworthy aspect of this study was the double-blind nature, ensuring anonymity. Students would not be aware of who they were marking, or were marked by. To enforce this completely, submitted code was obfuscated (java sources into byte code). One advantage of this is that it strips out identifying variable names and comments, which could identify other students. However, a downside of using byte code is that it can make it difficult to do in-depth analysis of the source structure which may make it harder to write complete test cases.\par %what do with this info?
The study identified two key features that assignments for a study of this nature need to have:
\begin{itemize}
 \item Well-defined interfaces
 \item Freedom for implementation
\end{itemize}
% put this here or in the planning section? From this, we can define some possible exercises that could be used during an evaluation of a peer assessment website.
This study found that the process of peer assessment was enjoyed by the students, and it ... %summarize here


\subsubsection{li-assessor-2010}
A study conducted by Li et at. \cite{li_assessor_2010} investigated which part of the peer assessment process is useful to the peers taking part: Being an assessor or an assessee. To study this, students were given the task of creating a webquest project. This was then marked by independent assessors, and the students were given a chance to provide feedback on the work of others. When the feedback was shared back, students were given another chance to improve their project, and it was marked again. The quality of the peer feedback itself was also checked by the independent markers.\par
The study found that 
\begin{quote}
there was a significant relationship between the quality of the peer feed-back the students provided for others and the quality of the studentsâ€™ own final projects
\end{quote}
This suggests that the actual exercise of providing feedback to others (acting as an assessor) is a worthwhile process for learning from. This study also found no evidence that the feedback itself was useful as a learning tool, suggesting that the act of giving feedback itself is more valuable.\par

\subsection{hooshangi-can-2015}
A study was conducted by Hooshangi, Weiss and Cappos \cite{hooshangi_can_2015} which investigated how the use of a security-based peer testing would help with teaching how to program securely. By setting assignments which would require the students to build programs defensively, with the mindset that fellow studetns would later try to break them.\par
This study found that  - not sure how much I can glean from this one

\subsection{farnqvist-competition-2016}
One interesting feature that could be implemented in a peer assessment website would be to integrate an automated testing framework. This would be able to give immediate (though limited in detail) pass/fail information, without having to wait for peers to add their own detailed feedback. The effectiveness of an automated testing tool (in this case not integrate into a competitive environment, not a peer assessment one) was investigated \cite{farnqvist_competition_2016}.\par
This study investigated how introducing a competitive element to a Data Structures and Algorithms CS course might help students to learn. The survey took place over two runs of the course over 2011 and 2012. In the 2011 course, lab contests were graded manually, and a voluntary contest used an automated grader. In the 2012 course, the lab assignments were marked using an automated grader for testing correctness and efficiency, and by human lab assistants to check code quality.\par
This test showed the attitudes of some students towards the use of an automated grader.

\subsection{sitthiworachart-effective-2004}
Peer assessment with an automatic test as a base for discussion and marking\cite{sitthiworachart_effective_2004}\par
Students, when asked, want anonymity [ I should blackbox or obfuscate]\par

\subsection{ruef-build-2016}
Build-it, Break-it, Fix-it\cite{ruef_build_2016}\cite{ruef_build_2015} is a programming contest that uses the concept of a Peer testing system to judge the success of various programming assignments, both in terms of general correctness and specifically in the context of security. This is somewhat different to the aim of an educational peer assessment website, however it is worth mentioning as ... why?
BiBiFi paper(s)\par

\subsection{sandbox}
If the website we aim to create is going to integrate built-in running of test cases we will need to make sure to use the right way of doing this.\par
There are apparmor, selinux and FBAC \cite{schreuders_empowering_2011}. But these are more focused on general applications running.\par
There is also the idea of running python directly in the browser - I think \url{http://pypyjs.org/} does this all in-memory... right? But problem there is you can't really store the test data, without getting it back in from the browser, then storing... and then you'd have to deal with it having to do possibly extensive operations in-browser blocking everything up (i tested this with a bogo sort, it doesnt work too well).\par
Another option is to use pypy sandboxing - but this is incomplete and doesn't look to be finished any time soon.\par

\subsection{mogus-impact-2012}
The use of a virtual learning environment (VLE) can be very valuable to students learning. Analysis performed by Mogus eet al. \cite{mogus_impact_2012} looked at data collected over two semesters of a (what kind) course. It then correlated the use of various tools offered by moodle (the VLE in use) against the success of students in that course (measured by their final grade). The analysis performed revealed that the interactions used within the VLE do have a positive influence on the eventual grades a student receives. It is worth noting this was not determined by using a control group, but by grouping students by their final grade, and then analysing the results.\par
Given that VLEs can be useful to students, and that the desired peer assessment system would be an online website, it seems that a logical aim would be to build a website that is modular enough that it may be integrated into such a VLE as Moodle \cite{moodle_about_2016}.

\subsection{keppell-peer-2006}
A research project was conducted by keppel et al.\cite{keppell_peer_2006} which involved some university lecturers re-structure their courses to involve more peer learning, with the aid of a virtual learning environment. 
Peer assessment should be in the form of feedback, they say, as making this part of the ggrade changes the dynamic.
study 1: using the vle to upload files, and then criquing files to improve them (in this case fashion designs) - students appreciated the immediate nature of feedback. students also appreciated the fact that this provided a more balanced feedback system than just a teacher telling them. 
I worry this one might be a bit too off-topic.
Related as the peer assessmnt website is in and of itself a sort of virtual learning environment, if a bit more restrictive than blackboard. (or not as the case may be, honhonhonhon)

\section{testing styles}
When considering the testing of software, there are several ways that this can be done. To produce a website that enables peer assessment, an appropriate testing methodology will need to be selected. Testing methodologies considered include:\cn
\begin{itemize}
 \item Unit testing - The use of xUnit style tests. Often used in test-driven development, these could be useful in detecting flaws if written post-development by a peer assessor
 \item Expected output testing - Running a program with some input and comparing the output to what is expected. The issue here is that the peer assessor first needs to know what the correct output is. Unlike unit testing, this would simply involve a series of inputs, and a series of expected outputs, and the task of checking the correctness from these tests is left to the website.
 \item Scenario Testing - Simulate an actual usage scenario. Assessors would use solutions as if they were 3rd party libraries, and develop their own programs that would use these as if in an actual use case. These programs would perform checks to make sure the solutions being tested were running as expected. This is more involved than unit or expected output testing, as it might be better placed to discover side effects of continued use of the solution.
 \item Property based testing - Using a proof checker, such as QuickCheck (or similar) to ensure that a program is acting correctly. This would place additional overhead onto the students as assessors, as they would need to learn and understand the annotations used by a proof checker.
\end{itemize}
In all cases, the solution being tested needs to conform to a standard interface. And in each case, the website running the tests needs to be able to extract some meaningful information. This is simpler in terms of unit and expected output, as these will offer output that will be easily readable (e.g. by diff, a count of pass/fail tests); scenario and property based tests would produce more verbose output, which while may be more informative, would be harder to analyse.

\chapter{Project Aims}
\section{Primary Aim}
The primary aim of this project is to prototype a website that will help to automate and enhance the process of peer assessment with relation to testing the correctness of code.
\subsection*{Sample use case}
A teacher will set a coursework assignment. This will be a task that requires a class with methods to be produced that conforms with a well-defined interface. Once completed, students will be able to submit solutions to the task to the website. Then, once enough submission are made, the process of peer assessment can begin. This will involve students testing the code of other students, but submitting test cases to the website. During this, students will also be able to see the code of other students. If they wish, students will be able to provide comments (feedback) to other students. Then, once the peer assessment process is complete, students will be able to see results of the peer assessment (test results, and any feedback) for their solution.

\section{Objectives}
\objitem{Website}{Build a website}{High Priority}{The prototype website will be used as the main interface which students will interact with.}
\objitem{Upload}{File upload for solutions}{High Priority}{The website will let students upload solutions to assignments.}
\objitem{Test}{Build test cases}{High Priority}{The website will let students build, and subsequently submit test cases in a simple way.}
\objitem{Run}{Run test cases}{High Priority}{Once the test cases are built, run them. The results of running them will be stored for later display. This is done asynchronously to the main interface, as running the test cases may take some time}
\objitem{View}{Viewing own solution}{High Priority}{Let the student see their own submission, which test cases have been run against it, the status of each test, any feedback from peer assessors}
\objitem{Results}{Display test results}{Medium Priority}{A submitted test case can run against all of the solutions. Display the results (passed, failed, error, feedback?, metadata?) on each.}
\objitem{Source}{Show source code}{Medium Priority}{A student should be able to read the source code, for example to inspect the coding style (variable names, structure etc.)}
\objitem{Feedback}{Add feedback}{Medium Priority}{Provide a framework for providing feedback to a specific coursework solution.}
\objitem{Sandbox}{Sandbox the system}{Low Priority}{As the website acts as an open interface for running python, it is important that it be secured against attacks (malicious or accidental).}
\objitem{Overview}{Overview screen}{Low Priority}{Provide a way for the teacher who set the coursework to see all of the active solutions and the tests run on each of them, as well as any feedback.}
\objitem{Modular}{Create a modular system}{Low Priority}{One motivation for building the system would be to plug it into other learning tools, such as moodle.}
\objitem{Record}{Record metadata about run solutions}{Low Priority}{When running solutions with test cases, record metadata such as time taken to complete, memory usage, code coverage, etc. Store this data for display with each test result.}

\subsection{Discussion of Objectives}
\subsubsection*{OBJ-Upload}
When the student is submitting a solution, this should be contained within a single file. For this iteration of the project, the upload system will focus on a coursework that is completed using python.
\subsubsection*{OBJ-Test}
As previously discussed in section \cn, possible implementations of this subsystem include: Unit, expected/actual i/o, property based and scenario testing. For the purposes of evaluating this prototype, we will focus on:
\subsubsection*{OBJ-Feedback}
An ideal implementation of this would provide a way for students to give feedback to their assessees outwith the confines of test cases. Possible methods of feedback could include:
\begin{itemize}
 \item Attaching comments to (the results of) test cases
 \item Annotating the source code of solutions (on a line by line basis)
 \item Providing general comments for a submitted solution
\end{itemize}


\chapter{Evaluation Strategy}

\subsection{Test Properties}
In order to test if the website is useful or successful, we wish to test for and see if the following properties are true:\\
\begin{tabularx}{\textwidth}{lX}
 \textbf{TST-Submit} & The process of submitting solutions (to a website vs. manually sharing files)\\
 \textbf{TST-Automate} & Automating the testing process (using the web interface vs. editing and running code manually)\\
 \textbf{TST-Enhance} & Enhancing the feedback given (seeing if the feedback given through a website is more useful than feedback that is given from manual testing and evaluation)\\
 \textbf{TST-Improve} & Improving the experience as a whole (a subjective check to see if the experience of using a website feels better than manual peer assessment)\\
\end{tabularx}

\subsection{Test Strategy}
In order to evaluate whether each test property has been met, a good strategy would be to:
\begin{enumerate}
 \item Use a simple programming exercise, e.g. a Binary Search Tree
 \item Have test subjects perform the process of peer assessment each others solutions without the website
 \item Test subjects respond to questionnaire regarding this process
 \item Perform a different programming exercise, e.g. An auto-balancing Binary Tree
 \item Have test subjects again perform a peer assessment but this time with the website
 \item Test subjects respond to questionnaire regarding this process
 \item Test subjects then (answer questionnaire about comparison OR attend discussion session)
\end{enumerate}
I feel the evaluation would need to be performed in this way (with subjects both using the website, and not using it), as in order to test \textbf{TST-Improve}, as you can only really gauge how the subjects feel about the website if they have tried both with and without it. The issue with testing in this manner is one of learning. As subjects will have completed one task already, their opinions and ability may be skewed by the second task. To mitigate this, we would want to swap the order in which both the tasks and the use of the website occurs (ideally this requires a minimum of 4 pairs of test subjects).\par
In order to test \textbf{TST-Submit}, this can easily be tested by A) Timing how long it takes subjects to complete the process of submitting files and test cases to the website vs a shared file system, and B) Asking the subjects which method they feel is faster.\par
\textbf{TST-Enhance} can be tested by A) asking the users which feedback they found more useful, and also by B) having the markers (in this case the evaluation test runner) assess and compare the feedback itself, on an objective level: How detailed is it, Is the feedback correct, etc.\par
\textbf{TST-Automate} is similar in some ways to \textbf{TST-Submit}, and could use the same evaluation methods, but in addition it is necessary to check that the tests themselves are actually correct and useful. It is possible that the website may guide and improve, or hinder, the production of meaningful test cases during the peer assessment. This could be evaluated in a similar fashion to the evaluation of \textbf{TST-Enhance}.

\subsection{Test Subjects}
The test strategy would require pairs of 2. At least 4 pairs (To count each combination of With/without website and task1/task2). It would be useful if the student has some prior knowledge of using python to program, but not be totally necessary. At least some proficiency in an object-oriented language would be needed.

\subsection{Ethical Analysis}
Copy in ethical thingy here, or as an appendix

\chapter{Development Plan}

\section{Requirements}
Based on the objectives outlined in section \textbf{wherever}, I can establish the following system requirements:\par
\begin{longtable}{ccc}
\textbf{ID} & \textbf{Description} & \textbf{Priority}\\\hline
F01 & Log users in & High\\
\end{longtable}

\section{Design}
Optional. uml diagrams, etc.

\section{Sequence of Operations}
\begin{sequencediagram}
\newthread{u}{User}
\newinst[2]{i}{Interface}
\newinst[2]{f}{Files}
\newinst[2]{s}{Shell}

\begin{sdblock}{Submission}{}
    \begin{call}{u}{Submit solution}{i}{ready}
        \begin{call}{i}{Store solution}{f}{errors?}
        \end{call}
    \end{call}
\end{sdblock}

\begin{sdblock}{Test Submit}{}
\begin{messcall}{u}{Submit test}{i}{ready}
    \begin{call}{i}{Store test}{f}{errors?}
    \end{call}
\mess[1]{i}{ready}{u}
\begin{call}{i}{Execute test}{s}{result}
    \begin{call}{s}{Get solution and test}{f}{files}
    \end{call}
\end{call}
\begin{call}{i}{Store result}{f}{errors?}
\end{call}
\end{messcall}

\end{sdblock}

\end{sequencediagram}


\chapter{Project Management}
\section{Timetable}
The following hard deadlines exist for the project:\\
\begin{tabularx}{\textwidth}{lX}
\textbf{Date} & \textbf{Task}\\
24 Nov 2016 & First Deliverable submitted as a PDF through Vision\\
25 Nov 2016 & Ethics approval submission made through project system\\
16 Dec 2016 & Interview with supervisor and second reader completed\\
17 Mar 2017 & Draft dissertation to supervisor for written feedback\\
24 Apr 2017 & Dissertation submitted\\
. & Dissertation submitted as a PDF through Vision\\
. & Dissertation and ZIP file of all files submitted through Project System\\
 5 May 2017 & Poster uploaded through Vision\\
. & Edinburgh Poster Session
\end{tabularx}\\
In addition to these, I have arranged the following self-imposed targets:\\
A gantt chart is included in the appendices.
\section{Risk management}
\riskitem{Eval}{Not enough users for evaluation}{Likely, Big Impact}{It may be the case that I am not able to get a substantial number of participants during the evaluation. In this event, I will have to... get more detailed info from the few participants that I do get...?}
\riskitem{No-Impl}{No usable implementation for evaluation}{Unlikely, Big Impact}{I may be unable to complete a usable implementation in time for the evaluation. If I suspect this is the case beforehand, I will scale back the scope and focus on implementing core functionality. In the worst case, a wizard-of-oz style system could be set up at the last minute.}
\section{Issues}
Detail professional, legal, ethical, etc. issues in this section.\\
If the evaluation test subjects are students, they will be taking part in the evaluation mid-semester. It is important that the impact of having taken part in the evaluation not have an adverse effect on their performance in classes.

\chapter{Conclusion}
Do I need this?

\bibliography{../Dissertation}{}
\bibliographystyle{plain}

\appendix
\section{Gantt Chart}



\end{document}
