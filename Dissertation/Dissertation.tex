\documentclass[a4paper,11pt]{report}

% PREAMBLE
% BEGIN

\usepackage[utf8]{inputenc}
\usepackage[backend=bibtex]{biblatex}
\bibliography{../Dissertation.bib}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{datetime}
\usepackage{url}
\usepackage{tikz}
\usepackage{pgf-umlsd}
\usepackage{pgfgantt}
\usepackage{pdflscape}
\usepackage{pdfpages}
% \usepackage{showframe}
\usepackage{geometry}
\geometry{
 a4paper,
 left=1in,
 top=1in,
}
\newcommand{\titles}{\\\vspace{1cm}}
\newcommand{\cn}{\textcolor{red}{[Citation Needed]}}
\newcommand{\objv}[3]{\item \textbf{OBJ-#1}: \textit{#2}\\#3}
\newcommand{\objitem}[4]{\begin{tabularx}{\textwidth}{lXr} \textbf{OBJ-#1} & #2 & #3\end{tabularx}\\#4\\}
\newcommand{\riskitem}[4]{\begin{tabularx}{\textwidth}{lcr} \textbf{RSK-#1} & #2 & #3 \\  \multicolumn{3}{X}{#4} \\  \end{tabularx}}
\newcommand{\objts}{\multicolumn{3}{X}{}\\}
\newcommand{\gi}[2]{\textbf{#1} - #2}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\eva}[2]{\textbf{EVA-#1} & #2\\}


\newcounter{FunCount}
\newcounter{NFunCount}
\newcommand{\freq}[3]{\addtocounter{FunCount}{1}F\arabic{FunCount} & OBJ-#1 & #2 & #3\\}
\newcommand{\nfreq}[3]{\addtocounter{NFunCount}{1}N\arabic{NFunCount} & OBJ-#1 & #2 & #3\\}

\pdfinfo{%
  /Title    (Final Year Dissertation)
  /Author   (Leon McGregor)
}


% END
% DOCUMENT START

\begin{document}
% BEGIN
\pagestyle{empty}

{\centering\Large
\includegraphics[width=0.4\textwidth]{../hwu.png}\titles
Final Year Dissertation\\
{\huge\bfseries Web Platform for Code Peer-Testing\titles}
MEng Software Engineering\titles
L\'eon \textsc{McGregor} - H00152968\titles
{\large\textit{Supervisor}\\}
Manuel \textsc{Maarek}\titles
{\large\textit{Second Reader}\\}
Andrew \textsc{Ireland}\\
\vfill
}

\newpage
{
  \renewcommand{\thispagestyle}[1]{}
  \begingroup
    \makeatletter
    % Redefine the \chapter* header macro to remove vertical space
    \def\@makeschapterhead#1{%
    %\vspace*{50\p@}% Remove the vertical space
    {\parindent \z@ \raggedright
        \normalfont
        \interlinepenalty\@M
        \Huge \bfseries  #1\par\nobreak
        \vskip 20\p@
    }}
    \makeatother

    \tableofcontents
  \endgroup
}
\newpage
\doublespacing


\section*{Declaration}
I, L\'eon McGregor confirm that this work submitted for assessment is my own and is expressed in my own words. Any uses made within it of the works of other authors in any form (e.g., ideas, equations, figures, text, tables, programs) are properly acknowledged at any point of their use. A list of the references employed is included.\par
Signed: L\'eon McGregor\par
Date: \today

\vfill

\section*{Abstract}
Undergraduate students in Computer Science courses take part in coursework exercises in the form of programming tasks. The primary aim of this project is to improve this process by facilitating peer testing and peer feedback.\par
To achieve this, I have implemented a prototype of a website that would combine the delivery of coursework assignments within Computer Science Programming courses, the self-testing of solutions to said coursework, and subsequently the peer testing that can take place after submission of the coursework is complete.\par
An evaluation of my prototype has revealed that \todo{...}

\vfill

\section*{Acknowledgements}
Thanks to Manuel Maarek for helping me to organise this project. Additionally, thanks are given to the members of the QAA project headed by Manuel and those who took part in the evaluation study for their input.

\newpage

\pagestyle{headings}
\markright{L\'eon McGregor - Final Year Dissertation}

% END


% INTRODUCTION
\chapter{Introduction}
\todo{make sure all notes from first report are resolved}
% descibe organisation of dissertation
% TODO summarise objectives
% problems solved
% methods of implementation and research
% results
% any big achievemtns
% any limits.
% cross reference each of these to their sections further within the report


\chapter{Background}
\section{Peer Assessment}
\subsection{What is it?}
Peer assessment is a process in which students assess each other. This opposes the more traditional stance where a teacher performs the assessment. As defined by topping \cite{topping_peer_2009},
\begin{quote}
``Peer assessment is an arrangement for learners to consider and specify the level, value, or quality of a product or performance of other equal-status learners.''
\end{quote}
That is to say, students with a similar level of education assessing the work of each other to give critical feedback and discussion. This could be done in many ways, such as between pairs or in groups, and can be performed on any number of different activities from programming exercises to oral reports.


\subsection{Why use it?}
Peer assessment is a process with many benefits to participants in education. Sadler \& Good \cite{sadler_impact_2006} have suggested the following concepts that peer assessment can help with
\begin{itemize}
 \item Peer assessment is more immediate, so students can get more feedback, and sooner
 \item Students performing marking can reduce the workload for teachers
 \item The process of checking and thinking about another students answer can improve a students own understanding
 \item Peer assessment can help students better understand testing and can become aware of their own strengths and weaknesses
 \item Following peer assessment students can gain an improved attitude towards the process of learning as a whole
\end{itemize}
From this, I can see that using peer assessment is definitely a desirable activity to include in the educational process.\par

Peer assessment can offer much help towards education of students, but it would be worthwhile to know just which aspects are the most useful. A study conducted by Li et at. \cite{li_assessor_2010} investigated the peer assessment process with the aim of discovering which part of it is most useful to the students involved: Being an assessor or an assessee. To study this, undergraduate student teachers were given the task of creating a \textit{WebQuest}\footnote{In this context, a \textit{WebQuest} project is an activity created by the student teachers that would be given to their students. Following the \textit{WebQuest} instructions, students are guided through Internet resources, and offered ``scaffolding activities'' to help them learn.} project. This was then marked by independent assessors, and the student teachers were given a chance to provide feedback on other student teachers \textit{WebQuest}s. Following this, the feedback was returned and students teachers were given another chance to improve their project, and it was marked again. The quality of the peer feedback itself was also checked by the independent markers. The study found that 
\begin{quote}
``there was a significant relationship between the quality of the peer feed-back the students provided for others and the quality of the students' own final projects''
\end{quote}
The findings of the investigation would suggest that the actual exercise of providing feedback to others (acting as an assessor) is a worthwhile process for learning from. This study also concluded that there was no reasonable link between the feedback itself as a learning tool, suggesting that the act of giving feedback itself is more valuable and that low quality feedback does not harm the learning experience.\par
From the results of this study, I can gather that the most effective part of peer assessment is the actual act of assessment itself. Therefore, in a system that helps students assess program code, I want to prioritise helping them to perform this assessment over making sure that the feedback the students can provide is useful. Through this, I can ensure that students make the most learning possible out of the system.


\subsection{How to do it?}

With the knowledge that peer assessment can be useful, it is important to know how a peer assessment should be conducted. A study performed in a classroom environment by Smith et al.\cite{smith_using_2012} focused more on the use of peer assessment as a tool for teaching testing of code, in addition to the existing course.\par
Over the course of this study, which took place using coursework from a 12-week university course, the following was completed for each coursework: Submitting solutions, then submitting peer reviews (which includes a description of the testing that they performed on another solution, and the results of this), and then a review of the peer assessment (including what was learnt, an evaluation of feedback on their own solution, and optionally a corrected solution).\par
One particularly noteworthy aspect of this use of peer assessment was the double-blind nature, ensuring anonymity. Students would not be aware of who they were marking, or were marked by. To enforce this completely, submitted code was obfuscated (java sources into byte code). One advantage of this is that it strips out identifying variable names and comments, which could identify other students. However, a downside of using byte code is that it can make it difficult to do in-depth analysis of the source structure which may make it harder to write complete test cases.\par
The study identified two key features that assignments for peer assessment need to have:
\begin{itemize}
 \item Well-defined interfaces
 \item Freedom for implementation
\end{itemize}
In addition to this, Smith has found that it was possible to integrate the peer assessment process without having to significantly alter the existing course material, and the students taking part enjoyed the experience. This shows promise, as it could indicate many Computer Science courses (that offer coursework meeting the requirements), could be modified to include their own peer assessment exercises.\par

Peer assessment can prove to be a very valuable experience for students. Falchikov has collected various case studies of past peer assessments \cite{falchikov_improving_2013}, and the following aspects can be found:
\begin{itemize}
 \item If the marking criteria are properly explained, there is often no significant difference between the marks awarded by students and those that would be awarded by teachers. This would tend to indicate that students do assess each other fairly.
 \item One of the most important aspects of peer assessment is the ability of the student to learn how to assess other students and from this learn how to critically assess and improve their own work.
 \item It is important to make sure students feel confident, otherwise they may not assess their peers as honestly as they might otherwise have done. Some students will feel conflicted about marking their peers, particularly if they might have to give low marks.
 \item During peer assessment more benefits may come from students assessing multiple solutions, rather than each focusing on one.
\end{itemize}

\subsection{Summary of Peer Assessment}
The process of peer assessment holds much value for all of the participants in the educational process, and can provide some skills that can be carried beyond students time in education. It can be used to provide more immediate feedback, help students to learn as they evaluate solutions and create this feedback, and could be integrated with some existing coursework assignments. Beyond just letting students learn about how to grade against a mark scheme, peer assessment can often teach students a lot about the subject material itself, letting them gain a deeper understanding about it. Peer assessment also helps to grow students ability to critically evaluate both their own, and others, work. Students who are to engage in Peer assessment should be given sufficient help so that they can perform the assessment with confidence, and work with multiple peers to get the full positive effects of peer assessment.


% END
% TECH + PLATFORM BACKGROUND
% BEGIN
\section{Technology-Enhanced Learning Platforms}
\subsection{Virtual Learning Environments}
There has been a rapid increase in the use of websites and Internet services in recent times, and many of these services can aid in student learning. One such style of website is the Virtual Learning Environment (VLE). Examples of such websites could include Moodle, \cite{moodle_about_2016} or Blackboard \cite{blackboard_blackboard_2016}. The use of a VLE can be very valuable to students learning, as they offer such features as integrated coursework submission systems, discussion forums, sections for storing and displaying class lecture notes and more.\par
Analysis performed by Mogus et al. \cite{mogus_impact_2012} looked at data collected over two semesters of a teacher education course. The study then collected bulk data regarding usage statistics from the various tools offered by Moodle (the VLE of choice in the study). During analysis, this data was correlated against the success of students in that course, as measured by their final grade. The analysis 
revealed that the interactions used within the VLE do have a positive influence on the eventual grades a student receives. It is worth noting this was not determined by using a control group, but by grouping students by their final grade, and then analysing the results.\par
Given that VLEs can be useful to students, and that the desired peer assessment system would be a website, it seems that a logical aim would be to build a peer assessment system that is modular enough that it may be integrated into such a VLE as Moodle.\par

To determine whether students can benefit from peer assessment in an online, \textit{technology-enhanced} environment, a research project was conducted by Keppel et al. \cite{keppell_peer_2006}. This project involved some university lecturers, who re-structured their courses to involve more peer learning, with the aid of a virtual learning environment (in this case, Blackboard).\par
The project took place over 3 different case studies. The different studies used journal, discussion and file sharing features of the VLE to enhance 2 courses in fashion design, art education and the design of a new learning website. Based on the evidence collected during these case studies, it was found that:
\begin{itemize}
 \item Students found this assessment to be fairer than just against teacher
 \item Students appreciated the ongoing peer critique performed through reflective journals
 \item Teachers felt the instant nature of feedback was very useful
 \item The support offered by the assisting technology of the VLE encouraged collaboration
\end{itemize}
The authors of the project also suggest that unless the peer assessment within the VLEs is a marked process (against a students grade), students may be unwilling to participate. These findings are quite useful, and are a good indicator that the positive aspects of traditional peer assessment are not hindered by using an online system (as opposed to in-person activities).\par

\subsection{Classroom Platforms}
A study by Phil Davies\cite{davies_computerized_2000} was conducted using a peer testing system in a university communications and networking module. This peer testing system aimed to help students easily provide feedback on reports, in addition to attempting to locate plagiarism.\par
The CAP (Computerised Assessment with Plagiarism) provided an interface to let students read the report of another student. They could then provide feedback on the report and follow references on the report to identify any \textit{copy \& paste} plagiarism. This CAP system is of particular interest as the web interface described in Davies' paper is quite similar to what I would aim to produce during this project. Although this Davies' system is more aimed at identifying plagiarism in reports than testing program code, I do feel that this is still relevant as it makes use of a quite involved peer assessment process.\par
This experiment followed the results students achieved over a sequence of 4 tasks:
\begin{enumerate}
\item A Report
\item A multiple choice test
\item A period for peer marking the reports between students
\item A final multiple choice test
\end{enumerate}
The experiment looked to see if the peer marking was at all helpful in improving the marks of students between the two tests. The results found that the peer marking was very useful for students who had initially performed poorly. Student comments on the experience would suggest that they found the peer assessment process both enjoyable and informative. There is also evidence from this feedback indicating the importance of maintaining anonymity, as some students felt it would be difficult to the marking non-anonymously.\par
From this, I can gather that the model of peer assessment used, through the web interface, was indeed successful. Based on the evidence from this study, the use of a similar system could well be helpful to students who would otherwise not have performed well. This study also revealed that students learnt a lot from the ``repetitive nature of the marking''. This could indicate that testers in such a system as mine might benefit from testing multiple solutions.\par
Given that the user interface and flow of information through the application provided by CAP is in many ways similar to my conceptual website, I would have liked to obtain a copy of it to evaluate further. Unfortunately, Davies has not made the source for the website available for download.\par
\begin{wrapfigure}{R}{0.4\textwidth}
\centering
\includegraphics[width=0.35\textwidth]{CAP.png}
\caption{\footnotesize A sample view of the CAP system, showing report viewer and marking guide.}
\end{wrapfigure}


In the 1990s, the University of Nottingham produced Ceilidh, a system that had the intent of automating and improving the assessment process for C Language Coursework. Later improvements were made by Heriot-Watt University adding support for
Standard ML (SML) \cite{foubister_automatic_1997}. This tool was used to guide students through the coursework, and then check the correctness of submissions. Ceilidh offered a skeleton answer to students, which pointed out any special language features that should be used. The submitted solutions are checked for correctness with a mixture of verifying output against a model solution, and checking the style of the code (e.g line count, use of certain function names, etc).  In particular for SML, students were asked to provide the types for written functions, as an additional test of understanding.\par
A study conducted by Foubister et al. investigated if such a system used for automatic assessment of students would be useful. To check this, the results of the final exam mark were compared to those marks from previous years where Ceilidh was not used. The study found that there was no real improvement made by Ceilidh, but most importantly there were no detrimental effects. Some positive side effects were observed: The time taken for students to receive feedback on the coursework was markedly decreased, and the tracking of progress offered by the tool allowed teachers to more easily identify any students that were having serious difficulties.\par
Given that I wish to implement a system that is more geared towards peer assessment rather than automated assessment, I think that the most valuable idea to draw from this would be the tracking that is offered to the teacher. Even though it is the students who will be doing the actual assessment, teachers should be able to see how students are progressing with these.\par

In 2002, M. Goldwasser started a peer testing initiative on a Data Structures course \cite{goldwasser_gimmick_2002}.  The main objective of this was to get students taking the course to submit, along with coursework, a test case to run on the program with the aim of finding bugs in other students programs. The main objective in this case was to focus learning on software testing, and the peer testing aspect was secondary, however the study does hold many parallels with my intended system design.\par
The submitted coursework solutions would be collected, along with each test case. Then, using an automated system each of the test cases would be run against each of the solutions. The major downside to this being that there is $x^2$ complexity in terms of the submission count.\par
Goldwasser offers some advice for potential future implementations of such an automated testing system:
\begin{itemize}
 \item To prevent some students falling behind due to having buggy input parsers, have the lecturer provide a standard implementation to to all students. Similarly, output needs to be handled in a standardised way.
 \item A limit should be placed on how large a test case a student can submit.
 \item The automated testing system should be able to handle the case where a solution works correctly, produces the wrong output, crashes, or does not terminate.
 \item Additionally, the testing system should be able to recover and restart from a given position without having to re-run all test cases, or re-run specific test cases if one of the submissions should change
\end{itemize}
I believe this advice is quite useful for our purposes. If building such a system, it is worthwhile to know and consider the various states a program could exit with when running the tests cases. This is still useful even if the implementation I create does not perform a full running of each test to each solution. The suggestion that the lecturer provide a way to handle input and output in a standard way is useful, but more so for lecturers who will set coursework assignments for the website.\par
Additionally, Goldwasser left the autograder used within his study open for download \cite{goldwasser_autograde_2002}. This script could be used as a starting point for implementation of running test cases. However, it does seem to offer more than I require, such as automatically grading (beyond just verifying test cases), and checking against a model answer.

\subsection{Contest Platforms}
As an aside to peer assessment, one optional feature of such an implementation could be to offer automated testing. If a lecturer so wished, they could use the system with a pre-submitted test case. In effect, acting as a peer to each student, so that they could receive test results immediately (or as quickly as the test completes) upon submission.\par
The effectiveness of an automated testing tool (as a sole means of feedback, outside peer assessment) was investigated Farnqvist \& Heintz \cite{farnqvist_competition_2016} within a Data Structures Course.\par
This study investigated how introducing a competitive element to a Data Structures and Algorithms CS course might help students to learn. The survey took place over two runs of the course over 2011 and 2012. In the 2011 course, lab contests were graded manually, and a voluntary contest used an automated grader. In the 2012 course, the lab assignments were marked using an automated grader for testing correctness and efficiency, and by human lab assistants to check code quality. The automated grader in use was \textit{Kattis} \cite{enstrom_five_2011}\par
This test showed the attitudes of some students towards the use of an automated grader. Students found this automated grading to be more fair than the manual equivalent. The study also found that ``students put in more effort in the course thanks to automated assessment''. This would suggest that allowing my implementation to be configured to run as an automated tester might be a worthwhile addition.\par
For the competitive element of this, the study found that competition positively affected student behaviour. The element that was most useful for the contest aspect of this was the autograder. The automated marking and tracking of marks proved useful in motivating students, by offering a leaderboard.\par


Build-it, Break-it, Fix-it\cite{ruef_build_2016} is a programming contest that uses the concept of a Peer testing system to judge the success of various programming assignments. This success is measured both in terms of general correctness and specifically in the context of security. This is somewhat different to the aim of an educational peer assessment website, however it is worth mentioning as the implementation used by the project is, I believe, a good starting model for my implementation of a web platform.\par
\begin{wrapfigure}{l}{0.4\textwidth}
\centering
\includegraphics[width=0.35\textwidth]{bibifi.png}
\caption{\footnotesize Basic diagram showing the architecture of the BiBiFi contest system.}
\end{wrapfigure}
The contest requires participants to \textit{build} a working solution that matches functional specifications, as well as being as secure as possible. If the solution can be built and the functionality verified by an automated system, then the team can move on to the next stage. Following this, testers will attempt to \textit{break} the security of the solution, and points will be allocated according to a zero-sum-game. The builders can then gain points by \textit{fixing} their solutions.\par
Throughout the contest, there is a central web-based infrastructure that manages running the contest website (with scoreboards, etc.), listening to builder git repositories, and also running and recording test results. The basic structure is as follows:
\begin{enumerate}
 \item There is a listener component that tracks changes to the git repo of the builders
 \item This pulls metadata to the contest database
 \item The database can activate and get results from the tester component
 \item The tester can use Amazon EC2 to create virtual machines which run the solution, and the tests, then check the results against a benchmark and oracle to see if all is correct (or not)
 \item After reporting this back through the tester to the database, the website front end will update any scoreboards
\end{enumerate}
The basic architecture described here seems to work very well for the contest, and I believe with some modification would be a good starting point for my own implementation. There is no need for git repositories, as the files are uploaded directly to the system. This could just represent the servers file system. Also, while a VM creation system might work well for the contest, I feel that this might be too costly, in monetary terms and time overhead. A simpler sandboxing solution might make more sense, particularly if the system is password protected and only used by students who probably don't want to risk losing coursework marks. (Additional security measures are discussed in \S 1.3.2).

\subsection{Summary of Tech-Enhanced Platforms}
There are many advantages, with respect to learning, brought forward by technology enhanced platforms. Some, such as web-based VLEs are more accessible, allowing students to take part in collaborative exercises from outwith the time and space constraints of a classroom. In Computer Science classrooms, technology is often used to aid in the automation of program testing. This can have positive effects such as giving quicker feedback on solutions and more accurately checking the correctness of solutions with many inputs. These tools could also provide more feedback to the teacher of a class, such as the progress being made by students, to help identify any problems. These systems can also be useful in contests, outwith classrooms and within. Within classrooms, contests using such systems can encourage students to perform better. Such systems can also evoke ideas for how a new technology enhanced learning system could be implemented. This includes the style of architecture, the interfaces that could be used and the general flow of the applications in use.\par



% END
% IMPLEMENTATION BACKGROUND
% BEGIN

\section{Implementation Considerations}
From the previous sections I can see that there is definitely a space for a website to aid in peer testing code. However, there are many issues to consider when choosing how the website should be developed and implemented.

\subsection{Anonymity}
One issue that can arise when students are interacting with each other from a perspective of evaluating each other is their anonymity. During a peer assessment / automated testing experiment performed by Sitthiworachart et al. \cite{sitthiworachart_effective_2004}, the students remarked about the use of anonymity. One students response suggested that in a non-anonymous peer assessment exercise, their marking would be influenced if they knew who it was they were marking. Another student said they would decline to take part if the peer assessment was not conducted anonymously. What can be drawn from these comments is that anonymity is clearly very important to those taking part in the peer assessment. But the opinions of anonymity might not correlate with the actual effect it has.\par

To study the effect that anonymity has, as opposed to just opinions about it, a study was conducted by Lan Li \cite{li_role_2016}. This study aimed to investigate just how effective anonymity is when it comes to making peer assessment more effective, and whether any negative impact from a lack of anonymity can be mitigated.\par
This quasi-experimental study was conducted with some in-training teachers, and aimed to see which is the most effective method of / using / while conducting a peer assessment exercise: Having assessors and assessees know each others identities, remain anonymous, or know identities while having received training. The training in this case involved watching a video that described some of the stresses and concerns that arise during peer assessment, and various forms of discussion regarding this.\par
One issue I have with this study is that it didn't cover the case of being anonymous and getting training. This was because the training was intended as a fallback for when anonymity is not possible. But this does invite the question of just how effective would anonymity be if training were offered as well.\par
The study found:
\begin{enumerate}
 \item ``Anonymity improves student performance in peer assessment''
 \item If anonymity can't be guaranteed, negative effects arising from this can be offset using training
 \item Anonymity does not reduce the pressure and tension related with peer assessment
\end{enumerate} 

From the evidence available, I can gather that there is a great importance in creating a peer assessment system which can function while maintaining anonymity. It might also be worthwhile integrating some form of training, as found in the study by Lan Li, as this might reduce stress during the peer assessment process, as it is not outwith the real of possibility that students might recognise the work of others even if it is anonymous.

\subsection{Security \& Sandboxing}
If the website I aim to create is going to integrate built-in running of test cases I will need to make sure to use the right way of doing this. Because the website I aim to build will accept solutions from students that will need to be executed, it is important that these applications be restricted in terms of the permissions available to them. Restricting how they can interact with the executing computer will prevent malicious attacks or accidental errors in code from damaging the whole system.\par
Some available security measures include AppArmor, SELinux, FBAC amongst others. A study by Schreuders \cite{schreuders_empowering_2011} investigated how easy it is to set up the relevant security measures. They found that the FBAC-LSM security model was the best in terms of the ease of setting up, and ensuring that programs worked correctly. The tool offered for FBAC-LSM security set up would be a good candidate for use in a peer assessment system for running programs.\par
There is also the idea of running programs directly in a browser, using a JavaScript interpreter in a way such as PyPy.js \cite{pypy.js_web_2016}. In this manner, the local file system would be protected, as the file system used by the python interpreter is virtual in memory. However this would not be an ideal solution, as if there were security risks they would simply be placed upon the assessor instead. Additionally, this model of execution would make it more difficult to get and store results of tests and would need to be run synchronously in the website UI. Long operations also block the browser from functioning (having tried this myself, I can see that this would be an issue).\par
Another method might be to modify source codes to use a language-specific sandboxing features. One example of such an interpreter/compiler is PyPy\cite{pypy_pypy_2016}, which is designed to offer some sandboxing. However, this is still in development and could not be relied upon for our purposes.\par
One issue with sandboxing security mechanisms is that such systems can only protect the system by blocking access certain API calls. To put it another way, sandboxes cannot in and of themselves identify malicious code. For this reason it might be worth integrating some form of anti-malware analysis as well.\par
As an alternative to sandboxing, one common method is to re-map the root directory to prevent an application from accessing files on the system that it should not. This is known as a chroot jail \cite{ubuntu_basic_2016}. This could be a useful way of restricting the solution submitted so that it can only access relevant files.

\subsection{Testing Submitted Solutions}
When considering the testing of software, there are several ways that this can be done. To produce a website that enables peer assessment, an appropriate testing methodology will need to be selected. Based on suggested testing styles from Bill Laboon \cite{laboon_friendly_2016}, some testing methodologies were considered for inclusion:
\begin{itemize}
 \item Linting - Performing basic checks for 'smells' of bad code design, such as identifying unused variables or methods. Not particularly useful from a perspective of checking code is correct, but is still useful in terms of producing good quality code.
 \item Unit testing - The use of xUnit style tests. Often used in test-driven development, these could be useful in detecting flaws if written post-development by a peer assessor. This would need the assessor to be familiar with how to write xUnit style tests, and how they should be structured, or it would not be viable.
 \item Expected output testing - Running a program with some input and comparing the output to what is expected. The issue here is that the peer assessor first needs to know what the correct output is. Unlike unit testing, this would simply involve a series of inputs, and a series of expected outputs, and the task of checking the correctness from these tests is left to the website.
 \item Scenario Testing - Simulate an actual usage scenario. Assessors would use solutions as if they were 3rd party libraries (a 'black box'), and develop their own programs that would use these as if in an actual use case. These programs would perform checks to make sure the solutions being tested were running as expected. This is more involved than unit or expected output testing, as it might be better placed to discover side effects of continued use of the solution.
 \item Property based testing - Using a proof checker, such as QuickCheck (or a similar implementation for languages other than Haskell) to ensure that a program is acting correctly. This would place additional overhead onto the students as assessors, as they would need to learn and understand the annotations used by a proof checker, but could provide a lot of coverage of possible inputs.
\end{itemize}
In all cases, the solution being tested needs to conform to a standard interface. And in each case, the website running the tests needs to be able to extract some meaningful information. This is simpler in terms of unit and expected output, as these will offer output that will be easily readable (e.g. by diff, a count of pass/fail tests); scenario and property based tests would produce more verbose output, which while may be more informative, would be harder to analyse and subsequently summarise. 

\subsection{Recording Feedback}
In addition to recording the results of test cases, to enable full peer assessment beyond just peer testing, the desired website should integrate methods of providing feedback through testers critical evaluation of solutions. There are many different ways this could be achieved however, beyond just receiving and recording a paragraph or two of text from the tester.
\subsubsection*{Inline Code Annotations}
One feature seen in Agile software development cycles is the Code Review \cite{github_project_2016}. This involves a colleague reviewing new code, and leaving comments for improvements. GitHub enhances this process by offering inline code annotations. These allow for feedback to be more targeted and, from this, likely easier to understand. This could be a useful feature to implement, as student testers could leave comments for the developers where necessary in the code.
\subsubsection*{Marking Against Criteria} 
To once again draw on research conducted by Falchikov \cite{falchikov_improving_2013}, they discuss peer assessment traits in terms of the criteria used to mark other students. There are different levels of input that students can receive while engaging in preparations for peer assessments. The teacher may provide all of the criteria for marking/assessing; there may be discussion between the students and teachers, where each decide what criteria are important; and lastly offering full control of the criteria to mark over to students. In an open ended answer situation, as is offered by programming exercises, this may be a good environment to encourage the student to come up with more of the criteria.\par
Given a criteria to mark against, it would be useful to integrate some simple systems to check that a given solution meets the defined criteria. Falchikov describes that some common methods used are Likert scales or checklists, working against a known criteria.

\subsection{Summary of Implementation Considerations}
Given the many different aspects covered above, I have identified the following key considerations when implementing the website:
\begin{itemize}
 \item A desirable outcome would be to have a website that can operate without any students knowing the identities of the students whose code they are testing and assessing.
 \item It will be important to impose some form of security on the website. There are many measures that could be taken here, including sandboxing mechanisms, that is restricting the operating system access of the executable; running an executable in a more secured environment, such as a jail; or including malware detection software.
 \item There is a choice of methods to employ when creating test cases. The test cases testers will write could include linting, unit testing and scenario testing, amongst others.
 \item Feedback could be collected from the testers through free comment in various places of the application, or by checking against a marking criteria.
\end{itemize}





% discuss related work in technical(?) literature & relevance to my project

% phase 1 [login, ]
% adaptations for phase 2 [multi-file, improved test matching plugins, prep for pipeline]



% Work from semester 1 from research report
% Work testing various technologies
% Initial bits of implementation
% Refining a working single-file prototype
% Project meeting \& Advancing to a multi-file prototype
% Evaluation planning, execution and results
% Post-Evaluation Improvements

\chapter{Initial Prototype}
\section{Choosing Tools}
\section{Initial Interface Design}
\section{Initial System Design}
\section{Initial Prototype Development}

\chapter{Discussions with Stakeholders}
\section{Additional Requirements Analysis}
\section{Modifications to system design}
\section{Modifications to interface design}
\section{Further development}
\section{Live Hosting}

\chapter{Evaluation Study}
\section{Methodology}
\section{Results}
\section{Have main objectives been met}

\chapter{Post-Evaluation Study Improvements}
\section{Any Modifications}


\chapter{Conclusion}
\section{Achievements}
what I have achieved according to objectives, and prior work done in the area
the main limitations
\section{Future work}
Discuss possible additions to allow this to be used within an actual learning context





% APPENDICES AND BIBLIOGRAPHY
%BEGIN
\singlespacing
\printbibliography
\addcontentsline{toc}{chapter}{Appendices}

\addcontentsline{toc}{section}{Ethical Analysis}
\section*{A: Ethical Analysis}
\textbf{Student}: L\'eon McGregor\\
\textbf{Title}: Web platform for code peer-testing\\
\textbf{Supervisor}: Manuel Maarek\\
\textbf{Abstract}: Develop a web platform for managing peer-testing and peer-feedback of programming code. The project aims at providing a user-friendly solution for giving and receiving feedback on programming artifacts.\\
\textbf{Purpose of Study}: The purpose of this study is to investigate the effectiveness of a prototype web platform for peer assessment of programming code. The test subjects will be asked to perform small programming exercises, and peer assess each others work. The test subjects will attempt this without and then with the prototype website, in order to see if the website is effective. Test subjects will also be given some questionnaires to fill out, and may participate in an extended discussion session if they choose.

\subsection*{Screening}
\textbf{Does the research involve any of the following?}: Human Subjects\\
\textbf{Interface Only Screening?}: No\\
\textbf{Full Ethical Screening?}: Other body not required

\subsection*{Use of Human Subjects}
\textbf{How will participants be recruited?}: Participants will ideally be students of computer science at Heriot-Watt. A general request for participants to join in will be sent out, perhaps through a university emailing list, or by posting flyers. This will be done up to a week before the study is conducted.\\
\textbf{All participants to be recruited are over 16, able to give informed consent, and have no known impediment that might affect their ability to participate in the study?}: Yes\\
\textbf{How long will participants have to decide whether to take part in the study?}: 7 Days\\
\textbf{Does the study involve actively deceiving participants?}: No\\
\textbf{Will participants be using non-standard hardware?}: No

\subsection*{Data Protection Compliance}
I confirm that, in accordance with Data Protection legislation:
\begin{itemize}
\item Indentifiable data will be stored on a secure machine with restricted access
\item Data will be anonymised before publication unless consent has been given
\item Identifiable data will only be retained for the duration of the consent granted by the participant
\item External data and systems will be used within the licence terms specified
\end{itemize}

\subsection*{Health and Safety}
I confirm that the project involves only standard IT equipment and exposes participants to no more hazards than a conventional office environment.


\addcontentsline{toc}{section}{Evaluation Consent Form}
\section*{B: Evaluation Consent Form}
\section*{C: Evaluation Survey Form}
\section*{D: Evaluation Study Task Descriptors}

\end{document}
% END
